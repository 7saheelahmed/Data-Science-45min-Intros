{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bandit Agorithms 101\n",
    "2015 August 28\n",
    "\n",
    "* See imports below for requirements\n",
    "* In the same directory you cloned Data-Science-45min-Intros, you should also clone the BanditBook repository from https://github.com/johnmyleswhite/BanditsBook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Agenda\n",
    "1. A simulation approach to building intuition about A/B testing\n",
    "2. Practical requirements of testing and operation\n",
    "3. Optimizing outcomes with multiple options with different payoffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. A simulation approach to building intuition about A/B testing\n",
    "\n",
    "###A/B Test:\n",
    "* Treatment (e.g. content presented)\n",
    "* Reward (e.g. impression, click, proceeds of sale)\n",
    "* Audience-in-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename='img/treat_aud_reward.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numpy.random import binomial\n",
    "from ggplot import *\n",
    "import random\n",
    "import sys\n",
    "plt.figure(figsize=(6,6),dpi=80);\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Simulating A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='img/ab.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each test is like flipping a fair coin N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='img/a.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"coin_toss\":binomial(1,0.5,100)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"coin_toss\":binomial(1,0.6,100)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"coin_%i\"%i:binomial(1,0.5,100) for i in range(20)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"coin_%i\"%i:binomial(1,0.52,100) for i in range(20)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Rewards for choosing A or B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 arm\n",
    "payoff = [-0.1,0.5]\n",
    "a = np.bincount(binomial(1,0.5,100))\n",
    "print np.dot(a, payoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Rewards for choosing A or B or C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2-arm, equal reward\n",
    "payoff = [0,1,2]\n",
    "a = np.bincount(binomial(2,0.5,100))\n",
    "print np.dot(a, payoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Simulating multiple options with different payoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payoff=[1,1.05]\n",
    "reward1 = np.bincount(binomial(1,0.5,100))[1] * payoff[0]\n",
    "print reward1\n",
    "reward2 = np.bincount(binomial(1,0.5,100))[1] * payoff[1]\n",
    "print reward2\n",
    "total_reward = reward1 + reward2\n",
    "print total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Can we differentiat two arms with different payoffs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trial(payoff=[1, 1.01]):\n",
    "    reward1 = np.bincount(binomial(1,0.5,100))[1] * payoff[0]\n",
    "    reward2 = np.bincount(binomial(1,0.5,100))[1] * payoff[1]\n",
    "    return reward1, reward2, reward1 + reward2, reward1-reward2\n",
    "\n",
    "trials = 100\n",
    "sim = np.array([trial() for i in range(trials)])\n",
    "df = pd.DataFrame(sim, columns=[\"t1\", \"t2\", \"tot\", \"diff\"])\n",
    "print len(df[df[\"diff\"] <= 0.0])\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we differentiat two arms with different probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trial(ps=[0.5, 0.51], payoff=[1, 1]):\n",
    "    reward1 = np.bincount(binomial(1,ps[0],100))[1] * payoff[0]\n",
    "    reward2 = np.bincount(binomial(1,ps[1],100))[1] * payoff[1]\n",
    "    return reward1, reward2, reward1 + reward2, reward1-reward2\n",
    "trials = 100\n",
    "sim = np.array([trial() for i in range(trials)])\n",
    "df = pd.DataFrame(sim, columns=[\"t1\", \"t2\", \"tot\", \"diff\"])\n",
    "print len(df[df[\"diff\"] <= 0.0])\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###More Arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='img/abcd.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"tot_reward\":binomial(2,0.5,100)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"tot_reward\":binomial(3,0.5,100)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trials = 100\n",
    "means = [0.1, 0.1, 0.9]\n",
    "reward = np.zeros(trials)\n",
    "for m in means:\n",
    "    # equal rewards of 1 or 0\n",
    "    reward += binomial(1,m,trials)\n",
    "df = pd.DataFrame({\"reward\":reward, \"fair_reward\":binomial(3,0.5,trials)})\n",
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Practical requirements of testing and operation\n",
    "\n",
    "###Explore vs Exploit?\n",
    "\n",
    "* Marketer is going to have a hard time waiting for rigorous testing as winners appear to emerge\n",
    "* Implement online system vs. testing system\n",
    "* Flavor: Chase successes vs. analyzing failures\n",
    "* Want to automate balance of explore and exploit and run continuously\n",
    "* Some danger in forgetting about significance and power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bandit Book Utilities \n",
    "from \"Bandit Algorithms\" by John Myles White\n",
    "\n",
    "Three utilities:\n",
    "* arms\n",
    "* algorithms\n",
    "* monte carlo testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../BanditsBook/python')\n",
    "from core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##3. Optimizing outcomes with multiple options with different payoffs\n",
    "\n",
    "###Epsilon Greedy\n",
    "\n",
    "1. Split on audience + context + creative with fraction\n",
    "2. Keep track of past results\n",
    "3. Split experiment and exploit offerings with probability epsilon\n",
    "\n",
    "Notes:\n",
    "* epsilon is the fraction of exploration\n",
    "* randomize everything all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "# Mean arm payoff (Bernoulli)\n",
    "means = [0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "# Mulitple arms!\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = map(lambda (mu): BernoulliArm(mu), means)\n",
    "print(\"Best arm is \" + str(ind_max(means)))\n",
    "\n",
    "t_horizon = 250\n",
    "n_sims = 1000\n",
    "\n",
    "data = []\n",
    "for epsilon in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    algo = EpsilonGreedy(epsilon, [], [])\n",
    "    algo.initialize(n_arms)\n",
    "    # results are column oriented\n",
    "    # simulation_num, time, chosen arm, reward, cumulative reward\n",
    "    results = test_algorithm(algo, arms, n_sims, t_horizon)\n",
    "    results.append([epsilon]*len(results[0]))\n",
    "    data.extend(np.array(results).T)\n",
    "    \n",
    "df = pd.DataFrame(data, columns = [\"Sim\", \"T\", \"ChosenArm\", \"Reward\", \"CumulativeReward\", \"Epsilon\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=df.groupby([\"Epsilon\", \"T\"]).mean().reset_index()\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x=\"T\",y=\"Reward\", color=\"Epsilon\"), data=a) + geom_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x=\"T\",y=\"CumulativeReward\", color=\"Epsilon\"), data=a) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Anealing Softmax\n",
    "\n",
    "Upgrades to $\\epsilon$-Greedy:\n",
    "* Need to run more experiments if rewards appear to be nearly equal\n",
    "* Keep track of results for exploration as well as exploitation\n",
    "\n",
    "Tempted choose each are in proportion to its current value, i.e.:\n",
    "\n",
    "$p(A) \\propto \\frac{rA}{rA + RB}$\n",
    "\n",
    "$p(B) \\propto \\frac{rB}{rA + RB}$\n",
    "\n",
    "Remember Boltzmann, and about adding a temperature, $\\tau$:\n",
    "\n",
    "$p(A) \\propto \\frac{-\\exp(rA/\\tau)}{(\\exp(rA/\\tau) + \\exp(rB/\\tau))}$\n",
    "\n",
    "$p(B) \\propto \\frac{-\\exp(rB/\\tau)}{(\\exp(rA/\\tau) + \\exp(rB/\\tau))}$\n",
    "\n",
    "And what is annealing?\n",
    "\n",
    "* $\\tau = 0$ is deterministic case of winner takes all\n",
    "* $\\tau = \\infty$ is all random, all time\n",
    "* Let the temperature go to zero over time to settle into the state slowly (adiabatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_horizon = 250\n",
    "n_sims = 1000\n",
    "\n",
    "algo = AnnealingSoftmax([], [])\n",
    "algo.initialize(n_arms)\n",
    "data = np.array(test_algorithm(algo, arms, n_sims, t_horizon)).T\n",
    "df = pd.DataFrame(data)\n",
    "#df.head()\n",
    "df.columns = [\"Sim\", \"T\", \"ChosenArm\", \"Reward\", \"CumulativeReward\"]\n",
    "df.head()\n",
    "a=df.groupby([\"T\"]).mean().reset_index()\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x=\"T\",y=\"Reward\", color=\"Sim\"), data=a) + geom_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x=\"T\",y=\"CumulativeReward\", color=\"Sim\"), data=a) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###UCB2\n",
    "\n",
    "Add a confidnece measure to our estimates of averages!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_horizon = 250\n",
    "n_sims = 1000\n",
    "\n",
    "data = []\n",
    "for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    algo = UCB2(alpha, [], [])\n",
    "    algo.initialize(n_arms)\n",
    "    results = test_algorithm(algo, arms, n_sims, t_horizon)\n",
    "    results.append([alpha]*len(results[0]))\n",
    "    data.extend(np.array(results).T)\n",
    "    \n",
    "df = pd.DataFrame(data, columns = [\"Sim\", \"T\", \"ChosenArm\", \"Reward\", \"CumulativeReward\", \"Alpha\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=df.groupby([\"Alpha\", \"T\"]).mean().reset_index()\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x=\"T\",y=\"Reward\", color=\"Alpha\"), data=a) + geom_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x=\"T\",y=\"CumulativeReward\", color=\"Alpha\"), data=a) + geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
