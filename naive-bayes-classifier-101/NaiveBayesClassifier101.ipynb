{
 "metadata": {
  "name": "",
  "signature": "sha256:89c1c13dac5e83406c13a0a552b6cdede95f8f625da03a5854915ffb2975a677"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Learning Naive Bayes Classification\n",
      "2014-07-22\n",
      "Scott Hendrickson (@DrSkippy)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuitively, some classification problems seem succeptible to a strategy based on the likelihood of our sample being in a given class as determined by the presense and strength of various features. For example, the likelihood of an email with the phrase \"Nigerian Prince\" being spam is much higher than an email with the phrase \"This is your monther. Why haven't you called?\"\n",
      "\n",
      "In terms of probabilities, we want to calculate $p(class | features)$ for each possible class and then choose the class with the max probability.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Bayes' Theorem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to proceed? Given a labeled training set, we can easily calculate $p(features|class)$. \n",
      "\n",
      "Flipping this over to get what we want looks like a job for Bayes' Theorem.\n",
      "\n",
      "$$p(x)p(y|x) = p(y)p(x|y)$$\n",
      "\n",
      "Use your algebra:\n",
      "\n",
      "$$p(features)p(class|features) = p(class)p(features|class)$$\n",
      "\n",
      "$$p(class|features) = \\frac{p(class)p(features|class)}{p(features)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Chain Rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This seems pretty straight forward.  There is one tiny hitch, thoough. There are potentially many dimensions to each feature vector, ${f_0, f_1, \\ldots ,f_n}$. This means we have,\n",
      "\n",
      "$$p(features|class) = p({f_0, f_1, \\ldots ,f_n}|c)$$\n",
      "\n",
      "Think about only 2 dimentions for a moment. If the features relate to the class independently, we can rewrite:\n",
      "\n",
      "$$p({f_0, f_1}|c) = p(f_1|c, f_0)p(f_0|c)$$\n",
      "\n",
      "But since the probablility of $f_1$ is independent of $f_0$, we can collapse to,\n",
      "\n",
      "$$p({f_0, f_1}|c) = p(f_1|c)p(f_0|c)$$\n",
      "\n",
      "So in general we can write:\n",
      "\n",
      "$$p({f_0, f_1, \\ldots, f_n}|c) = \\Pi^n_i p(f_i|c)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Naive Bayes Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start with a representative labeled training set. Calculate $p(c_i)$ for each class. Calculate $p(f_j)$ for all feature dimensions. Classify new samples by finding the most likely class given the features:\n",
      "\n",
      "$$p(class_i|{f_0, f_1, \\ldots, f_n})= \\frac{p(class_i)}{p({f_0, f_1, \\ldots, f_n})} \\Pi^n_i p(f_i|c)$$\n",
      "\n",
      "$$p(class_i|{f_0, f_1, \\ldots, f_n})= \\frac{p(class_i)}{\\Pi^n_i p(f_i)} \\Pi^n_i p(f_i|c)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Problem 1: Classify even/odd binary numbers by the number of 0s and 1s.\n",
      "\n",
      "* How many dimensions in the feature vector?\n",
      "* Why might this work? \n",
      "* Why shouldn't it work?\n",
      "* How well do you think this will work?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "truth = [\"odd\", \"even\"]\n",
      "training_data = [ \n",
      "        (\"{:05d}\".format(\n",
      "            int(str(bin(x)).replace(\"0b\",\"\")))\n",
      "            , truth[int(x%2 == 0)]) \n",
      "                for x in range(1,32)]\n",
      "training_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import itemgetter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_even = len([itemgetter(0)(x) for x in training_data if itemgetter(1)(x) == \"even\"])\n",
      "n_odd = len([itemgetter(0)(x) for x in training_data if itemgetter(1)(x) == \"odd\"])\n",
      "p_even = float(n_even)/len(training_data)\n",
      "p_odd = float(n_odd)/len(training_data)\n",
      "print \"p(even) = {}  p(odd) = {}\".format(p_even, p_odd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_zeros = sum([itemgetter(0)(x).count(\"0\") for x in training_data])\n",
      "n_ones = sum([itemgetter(0)(x).count(\"1\") for x in training_data])\n",
      "total_characters = n_zeros + n_ones\n",
      "p_zero = float(n_zeros)/total_characters\n",
      "p_one = float(n_ones)/total_characters\n",
      "print \"0s : {}  1s : {}\".format(p_zero, p_one)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "n_zeros_even = sum([itemgetter(0)(x).count(\"0\") for x in training_data if itemgetter(1)(x) == \"even\"])\n",
      "n_ones_even = sum([itemgetter(0)(x).count(\"1\") for x in training_data if itemgetter(1)(x) == \"even\"])\n",
      "p_zero_given_even = float(n_zeros_even)/(n_zeros_even + n_ones_even)\n",
      "p_one_given_even = 1.0 - p_zero_given_even\n",
      "print (\"p(0|even) = {}   P(1|even) = {} \".format(p_zero_given_even,p_one_given_even))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_zeros_odd = sum([itemgetter(0)(x).count(\"0\") for x in training_data if itemgetter(1)(x) == \"odd\"])\n",
      "n_ones_odd = sum([itemgetter(0)(x).count(\"1\") for x in training_data if itemgetter(1)(x) == \"odd\"])\n",
      "p_zero_given_odd = float(n_zeros_odd)/(n_zeros_odd + n_ones_odd)\n",
      "p_one_given_odd = 1.0 - p_zero_given_odd\n",
      "print (\"p(0|odd) = {}   P(1|odd) = {} \".format(p_zero_given_odd,p_one_given_odd))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Simplification  \n",
      "\n",
      "$p({f_0, f_1, \\ldots, f_n}) = const.$ for all calcuations, so we can omit it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def p_odd_given(sample):\n",
      "    n_zeros = sample.count(\"0\")\n",
      "    n_ones = sample.count(\"1\")\n",
      "    return p_odd * (p_zero_given_odd**n_zeros) * (p_one_given_odd**n_ones)\n",
      "  \n",
      "def p_even_given(sample):\n",
      "    n_zeros = sample.count(\"0\")\n",
      "    n_ones = sample.count(\"1\")\n",
      "    return p_even * (p_zero_given_even**n_zeros) * (p_one_given_even**n_ones)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct = 0\n",
      "for s, t in training_data:\n",
      "    print \"Pattern: {}\".format(s)\n",
      "    print \"  p(odd) = {:1.3f} p(even) = {:1.3f} predict = {:4s} truth = {:4s}\".format(\n",
      "            p_odd_given(s)\n",
      "            , p_even_given(s)\n",
      "            , truth[int(p_odd_given(s) < p_even_given(s))]\n",
      "            , t)\n",
      "    if truth[int(p_odd_given(s) < p_even_given(s))] == t:\n",
      "        correct += 1\n",
      "print \"=\"*15\n",
      "print \"Total correct: {} ({:2.2%})\".format(correct, float(correct)/len(training_data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Thoughts: \n",
      "\n",
      "* How should we think about choosing features for machine learning when we don't have any idea of mechanism?\n",
      "* Crazy to not use ideas of mechanism (i.e. do a 1 bit check on the LST of the binary number!)?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Problem 2: Classify tweets by Topics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make two sets of tweets with simple filters, don't work too much about what is in them.\n",
      "\n",
      "    ./search_api.py -f\"lang:en (dog OR cat)\" json | gnacs.py | cut -d\"|\" -f3  | sort -u | head -qn85 > pets.txt\n",
      "    ./search_api.py -f\"lang:en (hat OR coat)\" json | gnacs.py | cut -d\"|\" -f3  | sort -u | head -qn85 > hats.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = open(\"./pets.txt\",\"rb\").readlines()\n",
      "n_pets = len(corpus)\n",
      "corpus.extend(open(\"./hats.txt\",\"rb\").readlines())\n",
      "n_hats = len(corpus) - n_pets\n",
      "Y = [0]*n_pets + [1]*n_hats\n",
      "print \"corpus size = {}   n pets = {}   n hats = {}\".format(len(corpus), n_pets, n_hats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print corpus[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scikit Learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Like with all of the other other fundamental algorithm RSTs, don't use the code above. Instead, use Scikit Learn.  In this case, we take a big shortcut by using the term frequency inverse document frequency vectorizer (tfidf). Remember this weights each term by the number of appeances, but decreases the weights if it appears in too many documents to be informatived.  Scikit Learn uses sparse matricies for space efficiency."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vec = TfidfVectorizer(min_df=1)\n",
      "X = vec.fit_transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print corpus[101]\n",
      "print X[101]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "clf = MultinomialNB()\n",
      "clf.fit(X, Y)\n",
      "print(clf.predict(X[120]))\n",
      "print(clf.predict(X[12]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct = 0\n",
      "for i, doc in enumerate(corpus):\n",
      "    if Y[i] == clf.predict(X[i]):\n",
      "        correct += 1\n",
      "    else:\n",
      "        print i, Y[i], clf.predict(X[i]), doc\n",
      "print \"correct = {} ({:%})\".format(correct, float(correct)/len(corpus))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###How does it generalize?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "print \"PETS\"\n",
      "print clf.predict(vec.transform([\"My dog has fleas\"]))  # seems clear\n",
      "print clf.predict(vec.transform([\"Grab your cat and moat\"])) # also clear\n",
      "print clf.predict(vec.transform([\"This pet is a hamster\"])) # not in the original word list, but pet nonetheless\n",
      "print clf.predict(vec.transform([\"socks is the name of a pet\"])) # pet\n",
      "print \"OUTERWARE\"\n",
      "print clf.predict(vec.transform([\"Grab your hat and coat\"])) # obvious\n",
      "print clf.predict(vec.transform([\"My socks and shoes are dirty\"])) # obvious but too far?\n",
      "print clf.predict(vec.transform([\"These come in hard, tinfoil, thinking and mortar boards\"])) # tricky\n",
      "print clf.predict(vec.transform([\"Redhat's logo is a fedora\"])) # ok\n",
      "print clf.predict(vec.transform([\"The cat in the hat\"])) # mixed, confound it!\n",
      "print clf.predict(vec.transform([\"Hiding a haircut under there?\"])) # infered?\n",
      "print clf.predict(vec.transform([\"Mackinaw or Toque?\"])) # Too far?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}