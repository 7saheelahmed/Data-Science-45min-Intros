{
 "metadata": {
  "name": "",
  "signature": "sha256:708dd3b12132a18c8474e3608f2af184bfd27c39da3e34cb7794cecb0e6c2696"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# k Nearest Neighbors \n",
      "We are going to learn how to label some unlabled data based on *similar* characterisitcs to labled data. In terms of vocabulary, we are going to create a classifier for a specific target variable using feature comparisons. \n",
      "\n",
      "The starting point is the labeled dataset with various features. In this n-dimensional space, we then caluculate the distance between our unknown datum and it's k closests neighbors. \n",
      "\n",
      "The process has three main parts:\n",
      "\n",
      "- Optimize k\n",
      "- Distance calculation\n",
      "- Lablel logic\n",
      "\n",
      "This tutorial is uses two main resources:\n",
      " - [Machine Learning In Action](http://www.manning.com/pharrington/)\n",
      " - [Scikit-learn KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
      "\n",
      "### Optimize k\n",
      "A common question: Since k is an input, how do we choose the size of k?\n",
      "\n",
      "We can optimize k before we use kNN. This technique usually involves something like k-folds and testing various values of k, which let's us compare the mean error rates and choose \"the best\" k-value.   \n",
      "\n",
      "An alternative approach: optimize k while we use kNN. What if we let k vary (for each unknown) depending on the distribution of distances? A simple application would be to calculate all distances and k to be the number of neighbors within 1 standard deviation from the mean distance. \n",
      "\n",
      "In summary, optimizing k is a great idea; I don't have a great answer for you, but many people seem to like k-folds cross validation [[reference paper](http://lshtc.iit.demokritos.gr/system/files/XiaogangHan.pdf)]. I like the idea of letting k vary based on the relationship between the unknown datum and the labeled neighbors. \n",
      "\n",
      "### Distance calculation\n",
      "We can use euclidean distance or something differnt like using curved space. This choice is important as it directly orders the \"nearness\" of the neighbors. \n",
      "\n",
      "The main drawback to using kNN is that it can be rather huge memory suck.  \n",
      "\n",
      "### Label Logic\n",
      "A typical technique is to organize the k neariest neighbors labels as votes towards a certain label. The candidate with the most votes could win, but the label returned is entirely up to the author of the technique. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import operator\n",
      "\n",
      "def createDataSet():\n",
      "    \n",
      "    # features [boolean for poops in a litter box, average number of morning meows, average number of kisses per greeting]\n",
      "    dataSet = np.array([ [1.0,1.1,0],[1.0,1.0,1],[0,0,20],[0,0.1,3] ])\n",
      "    #labels = [0,0,1,1]\n",
      "    labels = np.array(['CAT','CAT','DOG','DOG'])\n",
      "    return dataSet, labels\n",
      "\n",
      "dataSet, labels = createDataSet()\n",
      "print \"group: \\n{}\".format(dataSet)\n",
      "print\n",
      "print \"labels: \\n{}\".format(labels)\n",
      "print \n",
      "print \"group.shape: \\n{}\".format(dataSet.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "group: \n",
        "[[  1.    1.1   0. ]\n",
        " [  1.    1.    1. ]\n",
        " [  0.    0.   20. ]\n",
        " [  0.    0.1   3. ]]\n",
        "\n",
        "labels: \n",
        "['CAT' 'CAT' 'DOG' 'DOG']\n",
        "\n",
        "group.shape: \n",
        "(4, 3)\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### ML in Action\n",
      "We can build our own custom kNN classifier based on code similar to that in [Machine Learning In Action](http://www.manning.com/pharrington/)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kNN(unknown,dataSet,labels,k):\n",
      "    \"\"\"\n",
      "    Apply kNN algorithm\n",
      "    \"\"\"\n",
      "    \n",
      "    # get length of dataset\n",
      "    dataSetSize = dataSet.shape[0]\n",
      "    \n",
      "    # tile creates a matrix the same size as dataSet with repeated rows of the unknown\n",
      "    diffMat = tile(unknown, (dataSetSize,1)) - dataSet\n",
      "    \n",
      "    # DISTANCE CALCULATION (eucledian distance from unknown to labeled points)\n",
      "    sqDiff = diffMat ** 2\n",
      "    sqDistances = sqDiff.sum(axis=1)\n",
      "    distances = sqDistances ** 0.5\n",
      "    \n",
      "    # sort the index of distances  \n",
      "    sortedDistIndex = distances.argsort()\n",
      "    \n",
      "    # LABEL LOGIC (here we use the shortest distance and votes are summed up to define the label)\n",
      "    classCount = {}\n",
      "    for i in range(k):\n",
      "        # grabs the labels the of the k nearest neighbors\n",
      "        voteLabel = labels[sortedDistIndex[i]]\n",
      "        \n",
      "        # tally the labels like votes\n",
      "        classCount[voteLabel] = classCount.get(voteLabel,0) + 1\n",
      "        \n",
      "    # sorts the tally (note: if it's a tie, then what?)\n",
      "    sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1), reverse = True)\n",
      "    print \"tally: \\n{}\\n\".format(sortedClassCount)\n",
      "    \n",
      "    # return the label with the most votes\n",
      "    return sortedClassCount[0][0]\n",
      "    \n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use the tally to lable the unknown\n",
      "print 'CUSTOM classification: \\n{}'.format(kNN([0,0,2], dataSet, labels,3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tally: \n",
        "[('CAT', 2), ('DOG', 1)]\n",
        "\n",
        "CUSTOM classification: \n",
        "CAT\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### scikit-learn \n",
      "We can use the [KNeighborsClassifier](docshttp://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) from scikit-learn to accomplish this task as well, but we have several algorithms from which we can choose.\n",
      "\n",
      "- `BallTree1` - \n",
      "- `KDTree1` - \n",
      "- `Brute` - \n",
      "- `Auto` - Atempts to choose the best algorithm.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use scikit-learn to classfiy\n",
      "from sklearn import datasets\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "# Create the kNN object \n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=3)\n",
      "\n",
      "# Use the known data\n",
      "knn.fit(dataSet, labels) \n",
      "\n",
      "# Predict the unknown\n",
      "print 'Scikit-learn classification: \\n{}'.format(knn.predict(np.array([0,0,1])))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "scikit-learn classification: \n",
        "['CAT']\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### to do:\n",
      "- k-folds example\n",
      "- real/practical example using kNN\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}