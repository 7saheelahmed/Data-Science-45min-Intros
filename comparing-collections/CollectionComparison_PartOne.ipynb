{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing collections (Part One)\n",
    "\n",
    "* Set comparison\n",
    "* Ordered collections\n",
    "* Ranked collections\n",
    "* Collection transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "import count_min\n",
    "\n",
    "# some matplotlib color-mapping \n",
    "cmap = plt.get_cmap('viridis')\n",
    "c_space = np.linspace(0,99,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create two sets of data scientists\n",
    "a = set(['josh','fiona','scotty','skippy'])\n",
    "b = set(['jeff','whitney','fiona'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.intersection(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = set(range(10))\n",
    "d = set(range(8,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.intersection(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.union(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an ordered sequence of evenly-spaced integers\n",
    "# create a second ordered seqence of intergers, which differs from the first by only noise\n",
    "a = np.array([[i,i+np.random.normal()] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the color variation shows the sequence order\n",
    "plt.scatter(a[:,0],a[:,1],c=c_space,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the the correlation between the sequences (off-diagonal elements), \n",
    "# which will be high for small noise\n",
    "np.corrcoef(a,rowvar=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now create two-similarly related sequences, but with non-even spacing and larger noise\n",
    "_ = [1,4,5,8,15,45,48,50,55,60,88,89,90,93,99]\n",
    "b = np.array([[i,i+np.random.normal()*5] for i in _])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(b[:,0],b[:,1],c=np.linspace(0,99,len(b)),cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the correlation\n",
    "np.corrcoef(b,rowvar=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now create randomly-ordered seqences with larger noise\n",
    "\n",
    "\n",
    "_ = np.array([random.random()*100 for _ in range(100)])\n",
    "c = np.array([[i,i+np.random.normal()*10] for i in _])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(c[:,0],c[:,1],c=c_space,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the correlation coefficient is still relatively large\n",
    "np.corrcoef(c,rowvar=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try relating with the nosie scale and sparsity of the sequences with the correlation coefficient.\n",
    "\n",
    "\n",
    "# Ordinal comparison\n",
    "\n",
    "e.g. comparing rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from search.api import Query\n",
    "import json\n",
    "import yaml\n",
    "creds = yaml.load(open('/Users/jkolb/.creds.yaml'))\n",
    "\n",
    "# set up a query to the Gnip Search API\n",
    "q = Query(creds['username'],\n",
    "          creds['password'],\n",
    "          creds['search_endpoint'],\n",
    "          paged=True,\n",
    "          hard_max = 1000,\n",
    "          search_v2 = True\n",
    "          )\n",
    "\n",
    "# query parameters\n",
    "start_date = '2016-08-01T00:00'\n",
    "end_date = '2016-09-01T00:00'\n",
    "rule = 'mom'\n",
    "\n",
    "# get the tweet data\n",
    "q.execute(rule,start=start_date,end=end_date)\n",
    "mom_tweets = list(q.get_activity_set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_frequency(tweets,num_top_terms = 30,delta=10**-5,epsilon=0.001,):\n",
    "    \"\"\"\n",
    "    Space-tokenize tweet bodies and \n",
    "    return exact and approximate 1-gram counts\n",
    "    \n",
    "    Approximate the counts with a count-min sketch\n",
    "    \"\"\"\n",
    "    \n",
    "    terms = collections.defaultdict(int)\n",
    "    sketch = count_min.Sketch(dict(delta=delta,epsilon=epsilon,k=num_top_terms))\n",
    "    for tweet in tweets:\n",
    "        for token in tweet['body'].split():\n",
    "            terms[token.lower()] += 1\n",
    "            sketch.update(token.lower(),1)\n",
    "    return (terms,sketch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_two_lists(list_1,list_2):\n",
    "    \"\"\"helper function\"\"\"\n",
    "    for x,y in zip(list_1,list_2):\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_top_terms = 30\n",
    "# accuracy parameters for CM sketch\n",
    "delta = 10**-4\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get exact and approximate top terms and counts\n",
    "mom_terms,mom_sketch = token_frequency(mom_tweets,num_top_terms,delta,epsilon)\n",
    "exact_top_mom_terms = list(reversed(sorted(mom_terms.items(),key = operator.itemgetter(1))))[:num_top_terms]\n",
    "approx_top_mom_terms = [(term,count) for count,term in reversed(sorted(mom_sketch.top_k.values(),key = operator.itemgetter(0)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kendall's tau coefficient** is a sort of correlation coefficient that is proportional to the difference between the number of _concordant_ pairs and the number of _discordant_ pairs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kt_result = scipy.stats.kendalltau(exact_top_mom_terms,approx_top_mom_terms)\n",
    "kt_result.correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_two_lists(exact_top_mom_terms,approx_top_mom_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A harder problem: how to account for the varying importance of rank?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over/under-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get some data around the term 'dad'\n",
    "\n",
    "rule = 'dad'\n",
    "q.execute(rule,start=start_date,end=end_date)\n",
    "dad_tweets = list(q.get_activity_set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dad_terms,dad_sketch = token_frequency(dad_tweets,num_top_terms,delta,epsilon)\n",
    "exact_top_dad_terms = list(reversed(sorted(dad_terms.items(),key = operator.itemgetter(1))))[:num_top_terms]\n",
    "approx_top_dad_terms = [(term,count) for count,term in reversed(sorted(dad_sketch.top_k.values(),key = operator.itemgetter(0)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display_two_lists(exact_top_dad_terms,exact_top_mom_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't really care about rank here. We care about removing the effect of a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_by_difference(term_counts,baseline_counts):\n",
    "    \"\"\"define a normalized term frequency that subtracts off a baseline count\"\"\"\n",
    "    normed_term_counts = {}\n",
    "    for term,count in term_counts.items():\n",
    "        try:\n",
    "            normed_term_counts[term] = count - baseline_counts[term]\n",
    "        except KeyError:\n",
    "            normed_term_counts[term] = count\n",
    "    return normed_term_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_results = normalize_by_difference(dad_terms,mom_terms)\n",
    "\n",
    "# look at top of list to see most \"dad\"-like terms\n",
    "list(reversed(sorted(normalized_results.items(), key = operator.itemgetter(1))))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and look at the bottom of the list to see the mom-like terms\n",
    "list(reversed(sorted(normalized_results.items(), key = operator.itemgetter(1))))[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this normalization-by-difference only works if the two term frequency distributions have the same scale of counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_by_fraction(term_counts,baseline_counts):\n",
    "    \"\"\"normalize counts by the difference of the term-fractions for each distribution\"\"\"\n",
    "    normed_term_counts = {}\n",
    "    B = sum(baseline_counts.values())\n",
    "    A = sum(term_counts.values())\n",
    "    for term,count in term_counts.items():\n",
    "        try:\n",
    "            # fraction of baseline distribution for this term\n",
    "            b_frac = baseline_counts[term]/B\n",
    "        except KeyError:\n",
    "            b_frac = 0\n",
    "        \n",
    "        # fraction of primary term frequency distribution for this term\n",
    "        a_frac = count/A\n",
    "        \n",
    "        factor = (a_frac-b_frac)\n",
    "        normed_term_counts[term] = count * factor \n",
    "    return normed_term_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_results = normalize_by_fraction(dad_terms,mom_terms)\n",
    "sorted_list = list(reversed(sorted(normalized_results.items(), key = operator.itemgetter(1))))\n",
    "sorted_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well, this sort of works. We could also comine the term-fractions in other ways, e.g. a ratio (a_frac/b_frac), or the relative difference ( [a_frac-b_frac]/a_frac ). \n",
    "\n",
    "In the end, we need to think harder about what differences and similarities we want to be able to highlight and ignore. See Part Two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
